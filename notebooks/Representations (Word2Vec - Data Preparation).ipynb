{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388477c2-ef87-4a13-9568-fa0ca1fac239",
   "metadata": {},
   "source": [
    "<img src='data/images/lecture-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2e81b-d466-4080-8885-6e36b2a06cb0",
   "metadata": {},
   "source": [
    "# Data Preparation: Word Embeddings (Word2Vec)\n",
    "\n",
    "Word embeddings are dense vector representations of words in a mathematical space, typically in high-dimensional space, where each dimension captures a different aspect of the word's meaning. These representations are learned from large amounts of text data using techniques like neural networks, specifically models like Word2Vec, GloVe, and fastText. The basic idea behind word embeddings is that words with similar meanings or usage patterns tend to occur in similar contexts. Therefore, by training a model to predict a word based on its surrounding words or predicting the surrounding words given a target word, the model can learn to encode semantic and syntactic relationships between words. Word embeddings have become a fundamental component in various NLP tasks, including sentiment analysis, machine translation, text classification, and information retrieval. They enable algorithms to better understand and work with textual data by representing words as continuous vectors with rich semantic information.\n",
    "\n",
    "Word embeddings are trained using unsupervised learning techniques on large amounts of text data. There are two popular approaches for training word embeddings: the count-based approach and the predictive approach.\n",
    "\n",
    "* **Count-based Approach:** In this approach, the co-occurrence statistics of words within a context window are calculated from the corpus. The context window is a fixed-size window of words surrounding the target word. The intuition behind this approach is that words that have similar contexts tend to have similar meanings.\n",
    "\n",
    "    * One popular count-based algorithm is GloVe (Global Vectors for Word Representation). GloVe constructs a co-occurrence matrix that captures the statistics of word co-occurrences in a corpus. It then factorizes this matrix to obtain word embeddings that encode the relationships between words based on their co-occurrence patterns.\n",
    "\n",
    "    * Another approach is LSA (Latent Semantic Analysis), which applies Singular Value Decomposition (SVD) on the co-occurrence matrix to obtain word embeddings.\n",
    "\n",
    "* **Predictive Approach:** The predictive approach uses neural network models to predict a target word based on its context or vice versa. These models are trained to minimize the prediction error and learn meaningful representations.\n",
    "\n",
    "    * The Word2Vec model, specifically the Skip-gram and Continuous Bag-of-Words (CBOW) architectures, are popular predictive models. Skip-gram aims to predict the surrounding words given a target word, while CBOW predicts the target word based on its context words.\n",
    "\n",
    "    * Recent models like ELMo (Embeddings from Language Models) and BERT (Bidirectional Encoder Representations from Transformers) use deep transformer architectures and contextual embeddings. These models learn word representations based on the entire context of a sentence, capturing more nuanced meanings.\n",
    "\n",
    "During training, the word embeddings are updated iteratively using techniques like stochastic gradient descent or negative sampling. The objective is to minimize a loss function that measures the discrepancy between predicted and actual words in the given context.\n",
    "\n",
    "In this course, we have a detailed look how **Word2Vec** embeddings are trained. Training word embeddings from scratch requires very large text corpora and is therefore very time and resource-intensive. Since we want to focus on the underlying approach, we train Word2Vec embeddings on small and domain-specific dataset. The purpose of this notebook is the prepared the text corpus -- the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) -- to serve as training dataset for both implementations of Word2Vec: CBOW and Skip-gram. As a reminder, the figure below , taken from the lecture slides shows the basic setup for both implementations:\n",
    "\n",
    "<img src='data/images/lecture-slide-06.png' width='80%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd05834",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961b41d-caad-4ac3-b52d-328c7c6b7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd765d-369b-4c5a-afc8-bbfbd100e3d5",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch as well as Torchtext, so we need to import the `torch` and `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11668d-9704-42bb-9135-fb69d27b8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94691ca",
   "metadata": {},
   "source": [
    "As usual, we rely on spaCy to perform basic text preprocessing and cleaning steps, mainly tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fabe1-764e-45d4-9306-82fecd0ec682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Tell spaCy to use the GPU (if available)\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b185fb-1a64-49ef-8cfb-00ca5a140530",
   "metadata": {},
   "source": [
    "Lastly, `src/utils.py` provides some utility methods to download and decompress files. Since the datasets used in some of the notebooks are of considerable size -- although far from huge -- they are not part of the repository and need to be downloaded (and optionally decompressed) separately. The 2 methods `download_file` and `decompress_file` accomplish this for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79521555-7228-4dfa-a67b-bbc1abf81929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import download_file, decompress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03237be0-90b7-402b-8cd6-e793a1d9980c",
   "metadata": {},
   "source": [
    "**Important:** The code cells below to download the file naturally include the URLs of the files. However, there is always the chance that one of those files might be removed or renamed, in which case the URL will now longer be valid. In this case, it is recommended to search for alternative links using, e.g., Google or Bing, which should cause no problems as all datasets used here are generally widely available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c030d-a96f-468d-a50e-9b44f6820137",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddf543-95f2-4903-bb0c-84d985653912",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "The [Large Movie Review Datase](https://ai.stanford.edu/~amaas/data/sentiment/), commonly known as the IMDb dataset or IMDb movie reviews dataset, is a widely used benchmark dataset in natural language processing (NLP) and sentiment analysis. Created by Andrew Maas and a group of researchers at Stanford University, this dataset consists of movie reviews collected from IMDb (Internet Movie Database).\n",
    "\n",
    "Here are the key characteristics of the Large Movie Review Dataset:\n",
    "\n",
    "* **Data Size:** It contains a collection of 50,000 movie reviews.\n",
    "\n",
    "* **Review Split:** The dataset is evenly divided into two sets:\n",
    "    * 25,000 reviews for training\n",
    "    * 25,000 reviews for testing\n",
    "\n",
    "* **Sentiment** Labels: Each review is labeled with sentiment polarity:\n",
    "    * 50% of reviews are labeled as positive\n",
    "    * 50% of reviews are labeled as negative\n",
    "\n",
    "* **Binary Classification Task:** The dataset is commonly used for binary sentiment classification tasks, where the goal is to classify whether a review expresses positive or negative sentiment.\n",
    "\n",
    "This dataset serves as a standard benchmark for sentiment analysis and text classification algorithms, enabling researchers and developers to evaluate and compare the performance of different machine learning and deep learning models in sentiment classification tasks. The availability of labeled data in large quantities allows for the training and evaluation of models to predict sentiment accurately, making it a valuable resource in the field of natural language processing and sentiment analysis research.\n",
    "\n",
    "Given its size, the dataset is not included in the Github repository. You can either download the dataset yourself using the link above, or you can run the notebook \"Representations (Word2Vec - Data Preparation)\" first which downloads the dataset for you.\n",
    "\n",
    "If you have already downloaded and decompressed the dataset in the previous notebook, you can skip the code cell below. Otherwise run the code cell to fetch the dataset. We recommend using the given `target_path` as this won't require any additional changes in subsequent code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec0a61-a220-4c0d-ad48-9fd89bc7badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Download file...')\n",
    "# download_file('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', target_path='data/datasets/imdb-reviews/')\n",
    "# print('Decompress file...')\n",
    "# decompress_file('data/datasets/imdb-reviews/aclImdb_v1.tar.gz', target_path='data/datasets/imdb-reviews/')\n",
    "# print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d291aa-a2e2-4c2b-a262-f3174f274dc3",
   "metadata": {},
   "source": [
    "The dataset comes organized in multiple folders, each folder containing many files where one file represents one review. For iterating over each review -- recall that each review is represented by its own file -- we can also prepared the folders for later code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88f9e2-6325-4dda-b6f4-a8d8adf27bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_base_path = 'data/datasets/imdb-reviews/'\n",
    "\n",
    "folders = [\n",
    "    corpus_base_path+'aclImdb/test/pos',\n",
    "    corpus_base_path+'aclImdb/test/neg',    \n",
    "    corpus_base_path+'aclImdb/train/pos',\n",
    "    corpus_base_path+'aclImdb/train/neg',\n",
    "    corpus_base_path+'aclImdb/train/unsup'    \n",
    "]\n",
    "\n",
    "num_reviews = 0\n",
    "\n",
    "for folder in folders:\n",
    "    num_reviews += sum([len(files) for r, d, files in os.walk(folder)])\n",
    "\n",
    "num_reviews = min(num_reviews, 999999999)    \n",
    "    \n",
    "print(\"Total number of reviews: {}\".format(num_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af03fc-b19a-437b-9dad-5b66d87f380f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f218e93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating CBOW and Skipgram Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643147c",
   "metadata": {},
   "source": [
    "### Auxiliary Method for Data Cleaning & Preprocessing\n",
    "\n",
    "The method `process_file()` below takes a single review file as input and returns all valid tokens as a list. This includes that the method removes all punctuation marks and stopwords. The method performs lemmatization, which is arguably not as obvious. The decision to lemmatize words when training word embeddings depends on the specific use case and the desired characteristics of the word embeddings. Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. For example, lemmatizing the words \"running,\" \"ran,\" and \"runs\" would result in the common lemma \"run.\" Lemmatization helps reduce the sparsity of the data and can group together different inflected forms of a word, which can be beneficial in certain scenarios. Here are a few considerations regarding lemmatization when training word embeddings:\n",
    "\n",
    "* **Reducing Dimensionality:** Lemmatization can reduce the dimensionality of the vocabulary and the resulting word embeddings. By mapping multiple inflected forms to a single lemma, you can potentially reduce the overall vocabulary size and improve the efficiency of the training process.\n",
    "\n",
    "* **Generalization:** Lemmatizing words can help capture the general semantic meaning of a word, as it removes specific tense, case, or number information. This can be advantageous if you want your word embeddings to capture broader semantic relationships.\n",
    "\n",
    "* **Preserving Word Variations:** On the other hand, if your application requires sensitivity to word variations, such as capturing different verb tenses or noun plural forms, you may choose not to lemmatize the words. By preserving the different forms, the resulting word embeddings may better capture the specific nuances or syntactic patterns associated with those variations.\n",
    "\n",
    "* **Task-Specific Considerations:** The choice of lemmatization may depend on the specific downstream task you plan to use the word embeddings for. Some tasks, like part-of-speech tagging or named entity recognition, may benefit from lemmatization to reduce word form variability. However, other tasks, like sentiment analysis or text classification, may require word embeddings that retain specific word forms to capture sentiment or emphasis associated with those forms.\n",
    "\n",
    "In summary, whether to lemmatize words during training depends on your specific requirements and the characteristics you want the resulting word embeddings to capture. Both lemmatized and non-lemmatized word embeddings have their own advantages and limitations, so it's essential to consider the specific needs of your application when making this decision. For the purpose of this and subsequent notebooks, we want to keep it simple and trying to minimize the vocabulary and therefore perform lemmatization\n",
    "\n",
    "Since the movie reviews can include HTML tags, we remove those as well using RegEx. Again, anything here is kept to a bare minimum to keep things short and simple. Feel free to put in more thoughts into potentially better preprocessing steps. We won't really use the trained word embeddings for any downstream task, so there is no harm trying different alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac51ab-9d3d-4cdb-afd3-fe4237008bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    text = None\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().replace('\\n', '')\n",
    "        \n",
    "    if text is None:\n",
    "        return\n",
    "\n",
    "    ## Remove HTML tags\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub(' ', text)\n",
    "    \n",
    "    ## Let spaCy do its magic\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ## Return \"proper tokens\" (lemme, lowercase, stopword removal)\n",
    "    return [ t.lemma_.lower() for t in doc if t.pos_ not in ['PUNCT'] and t.dep_ not in ['punct'] and t.lemma_.strip() != '' and t.is_stop == False ]\n",
    "\n",
    "\n",
    "process_file('data/datasets/imdb-reviews/aclImdb/train/pos/0_9.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f68a04",
   "metadata": {},
   "source": [
    "### Process Review Files\n",
    "\n",
    "The code cell below iterates over all text files representing the movie reviews in the specified folders, see above. For each review, we first extract all the tokens using the method `process_file()`. This returns the list of relevant tokens for this review which append to a list of all tokens across all reviews.\n",
    "\n",
    "For each token, we also keep track of its count. We only need this to later create the final vocabulary by only looking at the top-k (e.g., top-20k most frequent) words. For testing, we recommend using a lower value for `num_reviews` (e.g., 1000) to see if this and the other notebooks are working (of course, the results won't be great). Once you think all is good, you can set `num_reviews` to infinity to work on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27227dac-17de-46bd-967b-cd4175d8ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_num_reviews = min(num_reviews, 99999999)\n",
    "\n",
    "print(considered_num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5a896-b730-45bb-a804-c18c007f550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews(folders):\n",
    "    tokens = []                # List of all tokens\n",
    "    token_counter = Counter()  # Dictionary with all tokens and their frequencies\n",
    "    review_count = 0   # Running counter of process reviews\n",
    "    # Iterate over all reviews\n",
    "    with tqdm(total=considered_num_reviews) as progress_bar:\n",
    "        for folder in folders:\n",
    "            for file_name in os.scandir(folder):\n",
    "                # Ignore directories (just a fail-safe; not really needed)\n",
    "                if file_name.is_file() is False:\n",
    "                    continue\n",
    "                # Preprocess review\n",
    "                sentence_tokens = process_file(file_name.path)\n",
    "                # Add all extracted sentences to final list\n",
    "                tokens.extend(sentence_tokens)\n",
    "                # Update token counts\n",
    "                for token in sentence_tokens:\n",
    "                    token_counter[token] += 1\n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "                # Check if we need to stop early\n",
    "                review_count += 1\n",
    "                if review_count >= considered_num_reviews:\n",
    "                    return tokens, token_counter\n",
    "    # Return sentences and token counts\n",
    "    return tokens, token_counter\n",
    "                \n",
    "tokens, token_counter = process_reviews(folders)  \n",
    "    \n",
    "            \n",
    "print('Total number of tokens: {}'.format(len(tokens)))\n",
    "print('Number of unique tokens: {}'.format(len(token_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4233abd",
   "metadata": {},
   "source": [
    "It's important to note that `tokens` now contains all relevant tokens from all the considered reviews. In other words, we have concatenated all reviews into one long list of tokens. This also means that we completely ignore any sentence boundaries, which in turn means that the context of a word may belong to 2 sentences (or even more if the sentences are very short and the context window is large). There are different arguments why or why not this is a proper approach, but it's certainly not uncommon, and it keeps the code in this notebook simple. In practice, more thought goes into these design decisions for the data preparation. Here we can keep it simple by actually utilizing the fact that our corpus is very domain specific (movie reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817aeda0",
   "metadata": {},
   "source": [
    "### Create & Save Vocabulary\n",
    "\n",
    "For using the dataset to train a PyTorch model, we need to map each unique word/token to a unique index (i.e., integer identifier). Given a vocabulary size of `V` these unique indices must be of the range from `0` to `V-1`. This is needed since at the end, training a model using the data comes to matrix/tensor operations and we use identifiers to index the respective tensors. Also, we often want to do additional steps such as considering only the top-k most frequent tokens. Again, it's not difficult to implement this from scratch, however, the `torchtext` text simplifies this resulting in cleaner code.\n",
    "\n",
    "At least once we use all reviews, the number of unique tokens will be quite large. However, we already know that the vocabulary will contain many tokens that occurred maybe only once or twice. These tokens are not really useful for training word embeddings to begin with. We therefore only consider the most frequent tokens for the vocabulary. The method `process_reviews` already returns the number of occurrences for each token. So if we want to limit the total number of tokens, we simply need to pick the most frequent tokens using those counts. The code cell below accomplishes this, considering the top 20k tokens by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8450f-5aeb-4c3e-a5a8-92f8938956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 20000\n",
    "\n",
    "# Sort with respect to frequencies\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "token_ordered_dict = OrderedDict(token_counter_sorted[:TOP_TOKENS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e59dd-d564-4921-bb04-540dd494e652",
   "metadata": {},
   "source": [
    "We can now create a `vocab` object. In its core, it creates the mappings between the tokens and their indices. It also support some additional useful features:\n",
    "\n",
    "* For many tasks, we need to include special tokens in our vocabulary. For example, we often need a special token (e.g., `<PAD>`) to represent an \"empty\" word we can use to pad sequence (see also the other notebooks). Even more common is a special token (e.g., `UNK`) to represent tokens that haven't been seen when building the vocabulary. Not that the exact string for those tokens do not matter. For example, we could have used, say, `[[[padding]]]` and `[[[unseen]]]`. It's only important that those tokens are unique. In the code cell below, we also add `<SOS>` (start of sequence) and `<EOS>` (end of sequence). These are typically required for tasks such as machine translation. While not needed here, it's no harm having them either.\n",
    "\n",
    "* By using `set_default_index()` we can specify the default index to be used if a sentence we want to transform contains a word not seen before. Most intuitively, we will use the index representing the special token `<UNK>`.\n",
    "\n",
    "Strictly speaking, for training the word embeddings, only the `<UNK>` token is required. However, adding the other special tokens does not negatively affect the training, and adding those tokens could come in hand if we want to use the vocabulary and dataset for other training task where the tokens a required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876c2dc-3287-48ad-86cb-523114cbe735",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[UNK_TOKEN])\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07297059-b341-4ca2-a035-df784ecc281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file_name = corpus_base_path+\"imdb-word2vec-{}.vocab\".format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25519e",
   "metadata": {},
   "source": [
    "### Generate Dataset from Extracted Tokens\n",
    "\n",
    "Recall, that `tokens` contains all the tokens from all the reviews in a single list without any consideration of sentence boundaries. As such, we can simply move a sliding window over the whole list to capture the current context and the current center word. We create both datasets as Numpy arrays containing the indices of the context words and the center word. Recall from the lecture that a context and center word results in (a) on sample for the CBOW datasets and (b) `2*window_size` samples for the Skip-gram dataset. The figure below shows the relevant part from the lecture slides\n",
    "\n",
    "<img src='data/images/lecture-slide-07.png' width='50%' />\n",
    "\n",
    "The code cell below uses a loop to move a sliding window over all tokens to generate the CBOW and Skip-gram samples as illustrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1ad0f-6845-4c43-b573-8ef841f2e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "# Given the window size, we can directly infer the required sizes for the 2 Numpy arrays\n",
    "cbow = np.zeros(( len(tokens)-(2*window_size) , (2*window_size)+1 ), dtype=np.int32)\n",
    "skipgram = np.zeros(((len(tokens)-(2*window_size))*(2*window_size), 2), dtype=np.int32)\n",
    "\n",
    "# Loop through list of tokens\n",
    "with tqdm(total=cbow.shape[0]) as pbar:\n",
    "    for center_idx, pos in enumerate(range(window_size, len(tokens)-window_size)):\n",
    "\n",
    "        # Get current center word and current context words\n",
    "        center = tokens[pos]\n",
    "\n",
    "        context = tokens[pos-window_size:pos] + tokens[pos+1:pos+window_size+1]\n",
    "\n",
    "        # A CBOW sample is an array containg 2*window_size context word indices + the center word index\n",
    "        cbow_sample = np.array( vocabulary.lookup_indices(context) + vocabulary.lookup_indices([center]) )\n",
    "\n",
    "        cbow[pos-window_size] = cbow_sample\n",
    "\n",
    "        # Loop over all context words to generate the 2*window_size (center_word, context_word)-pairs\n",
    "        for idx, c in enumerate(context):\n",
    "            skipgram_sample = np.array(vocabulary.lookup_indices([center]) + vocabulary.lookup_indices([c]) )\n",
    "            skipgram[(center_idx*window_size*2)+idx] = skipgram_sample\n",
    "            \n",
    "        # Uupdate progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c848c4e",
   "metadata": {},
   "source": [
    "Again, we save our datasets to be later used in the training notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d48126",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(corpus_base_path+'imdb-dataset-cbow.npy', cbow)\n",
    "np.save(corpus_base_path+'imdb-dataset-skipgram.npy', skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf01473-e406-4948-be13-6b57d965ae66",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925207ff-f196-4783-9ebc-70726c43bfa1",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "As mentioned in the beginning, using the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train word embeddings has some clear limitations. As our goal is not to train word embeddings to be used for downstream tasks, but to understand and replicate the basic strategies, this is not an issue here. However, it's worthwhile to highlight some of the limitations and to address them when it comes to training \"proper\" word embeddings.\n",
    "\n",
    "* Apart from its small size, the dataset used here is very domain-specific, containing only movie reviews. This means that many words -- more common in other domains -- might not appear at all, or that words with multiple meanings will only be used in a single context. So while we could use word embeddings trained over this dataset for our sentiment classifier over the same data (see the notebook covering the RNN-based sentiment classification model), they arguably are not suitable for tasks in different domains.\n",
    "\n",
    "* The dataset used in this notebook was small enough that we could easily load the into the main memory. However, datasets to build proper language models are huge and would not fit into the memory all at once. In this case, some logic is required to first split the whole dataset into multiple chunks (e.g., different files) and then iterate over all chunks within each epoch.\n",
    "\n",
    "* In practice, large language models are typically trained in a distributed setting such as computing clusters housing many CPUs. Deep learning frameworks such as PyTorch and Tensorflow support distributed training and inferencing out of the box, but again, some additional logic is required to facilitate this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d393f3b-e0b5-44e2-81e7-0abc51017eb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306321b-65b2-471e-a9da-addd88515587",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preparation for training word embeddings is a crucial and challenging step in NLP tasks. Word embeddings, which represent words as dense vector representations in a high-dimensional space, have become an essential tool for various NLP applications such as machine translation, sentiment analysis, and named entity recognition.\n",
    "\n",
    "The importance of data preparation lies in the fact that word embeddings heavily rely on the context in which words appear. Therefore, the quality and diversity of the training data greatly impact the resulting word embeddings. To ensure accurate and meaningful representations, the data needs to be large, diverse, and representative of the target domain. This often requires extensive preprocessing, including text cleaning, normalization, tokenization, and removal of stopwords, punctuation, and special characters.\n",
    "\n",
    "Additionally, the challenge in data preparation arises from the inherent complexity of natural language. Language is highly nuanced, with varying sentence structures, grammar rules, and word meanings. Ambiguities, homonyms, and polysemy further complicate the task. Furthermore, handling out-of-vocabulary (OOV) words that do not appear in the training data requires special attention. Techniques such as subword tokenization or incorporating external resources like pre-trained embeddings can help address this challenge.\n",
    "\n",
    "Moreover, data preparation for training word embeddings needs to consider the size and quality trade-off. Large datasets can improve the coverage and generalization of embeddings, but they also require significant computational resources and time for processing. Balancing the need for a sufficiently large corpus with limited resources is a constant consideration.\n",
    "\n",
    "In conclusion, data preparation for training word embeddings is critical for generating meaningful and accurate representations of words. It involves cleaning and preprocessing the data to ensure its quality, diversity, and representativeness. The challenges lie in the complex nature of language, including nuances, ambiguities, and OOV words. Striking a balance between dataset size and quality is also a key consideration. Effective data preparation is essential for achieving high-quality word embeddings and subsequently improving the performance of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf7aef-97d8-4c20-9492-53361961492e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
